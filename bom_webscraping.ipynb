{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import requests\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scraping box office mojo\n",
    "scraped = pd.DataFrame() # instantiate an empty dataframe\n",
    "for i in range(1970,2021,1): # we want box office data for 1970 - 2020\n",
    "    print(i)\n",
    "    time.sleep(random.randrange(1,5,1))\n",
    "    page = requests.get(\"https://www.boxofficemojo.com/year/world/{}/\".format(i)) # go to the box office page for our year\n",
    "    soup = BeautifulSoup(page.content, 'html.parser') # get page content\n",
    "    for i in range(100): # we want the top 100 movies of each year\n",
    "        try:\n",
    "            href = soup.select(\"#table .a-link-normal:not(.a-nowrap)\")[i]['href'] # get the url for the specific movie\n",
    "            movie_title = soup.select(\"#table .a-link-normal:not(.a-nowrap)\")[i].text\n",
    "            time.sleep(random.randrange(1,5,1))\n",
    "            page = requests.get(\"https://www.boxofficemojo.com{}\".format(href)) # go to specific movie data url\n",
    "            soup2 = BeautifulSoup(page.content, 'html.parser') # get page content\n",
    "            href = soup2.select(\"table .a-link-normal\")[0]['href'] # get link for domestic data because it has daily data\n",
    "        except:\n",
    "            print('Movie {} errored out in try block 1'.format(i + 1))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            time.sleep(random.randrange(1,5,1))\n",
    "            page = requests.get(\"https://www.boxofficemojo.com{}\".format(href)) # go to domestic data url\n",
    "            soup3 = BeautifulSoup(page.content, 'html.parser') # get page content\n",
    "            genres = soup3.select(\".mojo-summary-values div\")[6].text.replace('\\n','') # get movie genres\n",
    "            genres = re.sub(r'\\s+',' ',genres) # clean genre text\n",
    "            genres = re.sub('Genres', 'Genres ', genres) # clean genre text, ready for list\n",
    "            genres = genres.split() # make into list\n",
    "            genres = genres[1:] # get rid of the index that say genres\n",
    "        except:\n",
    "            print('Movie {} errored out in try block 2'.format(i + 1))\n",
    "            continue       \n",
    "\n",
    "        try:\n",
    "            data = [] # instantiate empty data list\n",
    "            # regex pattern to capture wanted information\n",
    "            pattern = r'([a-zA-Z]{3}\\s\\d{1,2},\\s[12][09]\\d{2})(?:.*Day|.*Eve|Easter Sunday|Thanksgiving|Halloween)?([a-zA-Z]*)([\\d]*)\\$([\\w,]*)(-|(?:[+-<][\\d\\.,]+))[%]?(-|(?:[+-<][\\d\\.,]+))[%]?([\\d,]+)\\$([\\d,]+)\\$((?:[\\d]{0,3},)*[\\d]{3})(\\d{1,4})'\n",
    "            for x in soup3.select(\"#table tr\")[1:]: # loop through each daily box office row\n",
    "                string = x.text # get only the text\n",
    "                data.append(list(re.findall(pattern, string)[0])) # use regex to get the info we want\n",
    "            # turn data into data frame and append to our dataframe\n",
    "            df = pd.DataFrame(data, columns = ['date','dow','rank','daily_revenue','+/-YD','+/-LW','theaters','theater_avg','revenue_to_date','week'])\n",
    "            df['movie_title'] = movie_title\n",
    "            df['genre'] = [genres for x in range(df.shape[0])]\n",
    "            scraped = pd.concat([scraped.copy(), df], ignore_index = True)\n",
    "        except:\n",
    "            print('Movie {} errored out in try block 3: data is weekly instead of daily'.format(i + 1))\n",
    "            continue\n",
    "        print(i+1)\n",
    "        scraped.to_csv('scraped.csv') # save the info after every iteration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
